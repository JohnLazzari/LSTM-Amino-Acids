# LSTM for Amino Acid Sequences
Programming assignment 2 based on RNNs.

## Creating the datasets
We used `sequences.py` to create the datasets. The script has two optional flags: `--output-dir DIR` specifies where to save the datasets and `--full` processes the entire dataset (with the default being 1%). The script creates the files `train.npy` and `val.npy`.

## Training the model

## Experiments
The experiments were run using the following scripts.

### Long Term Dependencies
Long term dependencies were tested using `long-term-test.py`. The script loads the trained model. Then, it iteratively changes an amino acid in the sequence `k` positions from the prediction until a threshold of `10%` change in the prediction probability is not broken.

### Correctly Predicted Sequences
The table in Problem 3.2 was generated using `first-k-test.py`. The script iterates through the validation dataset to predict the rest of a sequence given the first`k` amino acids. The amount of correct predictions are recorded in the table. 

### Train the Model
Use `LSTM_train.py` to train the model on the padded 100 sequecences of amino acids computed in `sequences.py`. We use a stacked LSTM with two fully connected layers to make predictions on the next amino acid in the sequence.

### 3-Gram Models
The `3_gram.py` script generates a 3-gram model on the training set by counting the amount of sequences present and dividing by total amount. This is compared to the 3-gram model generated by the trained model, in which we generate a sequence and mulitply the sequence probabilities using the chain rule of probability.

### Generate Sequences
The script `generate_sequences.py` uses the LSTM to generate new sequences.
